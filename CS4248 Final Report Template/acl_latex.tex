% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SciCite Final Report}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{A0251134U, A0251434M, A0222300H, A0251155M, A0251403W \\
  Group 14 \\
  Mentored by Hai Ye \\
  \texttt{\{e0950138, e0950438, e0559832, e0950159, e0950407\}@u.nus.edu}}

\begin{document}
\maketitle
\begin{abstract}
This document is a template for the NUS CS4248 Natural Language Processing group project final report, which is limited to eight A4-sized pages for the main report body. This document contains the best practices for the group project final report, a marking rubric (\S\ref{s:best}) and formatting instructions for the report (\S\ref{s:format}).  It also includes a list of frequently asked questions about CS4248 projects, included as Appendix B. 
The document itself conforms to its own specifications, and is therefore an example of what your report should look like.  

You can (and should) access the live version of this file through the URL below:
\url{https://www.overleaf.com/read/spjdcnpvcgyk}.  This version you're reading is version 230312 (2023 March 12).
\end{abstract}

\section{Report Best Practices}
\label{s:best}

Your group report is {\bf strictly} limited to eight pages\footnote{For the main body of the report: title, abstract and main sections. Backmatter does not count towards this limit.} and summarises all of your group's understanding of your problem, models, experimental results and insights.  Typically, such (empirical/experimental) scientific reports for natural language processing follows a five- to six-section format ({}suggested length in parentheses, for a {\bf eight}-page limit; do not feel compelled to match these suggested lengths), as follows:

\section{\bf Introduction}
\section{\bf Related Work / Background}
\subsection{Citation intent classification}
Recently, citation intent classification has gained a lot of attention because it is critical for machine reading of individual publications and automated analysis of the scientific literature. The first related dataset (ACL Anthology Reference Corpus) is proposed by Steven et.al (2008) \cite{220fe80c8f374e458d79d387be245c5f}. After that, Arman et.al (2019) \cite{cohan2019structural} proposed a dataset (SciCite) which is five time large and covered multiple scientific area compared with existing dataset. Based on this dataset, we can better train the model.
\subsection{Elmo \cite{peters-etal-2018-deep} }
Matthew et.al (2018) \cite{peters-etal-2018-deep} proposed a novel way, Elmo, to represent the words into a vector or embeddings which can generate context-sensitive word embeddings that capture the syntactic and semantic nuances of words in their context. 
ELmo \cite{peters-etal-2018-deep}  is based on a two-layer BiLSTM\cite{650093}. First, a character-level convolutional neural network is used to convert the words of a text string into raw word vectors, and then the raw word vectors act as inputs to the first BiLSTM\cite{650093} layer. The forward pass contains information about a certain word and the context (other words) before that word and the backward pass contains information about the word and the context after it. This pair of information, from the forward and backward pass, forms the intermediate word vectors.
These intermediate word vectors are fed into the next layer of biLSTM\cite{650093}. The final representation (ELMo \cite{peters-etal-2018-deep} ) is the weighted sum of the raw word vectors and the 2 intermediate word vectors.
\subsection{GloVe\cite{pennington2014glove}}
Proposed by Pennington et.al (2014) \cite{pennington2014glove}, GloVe has been widely used in word embedding. The idea of GLoVe\cite{pennington2014glove} is to learn the vectors by conducting dimensionality reduction on the co-occurrence counts matrix and become a word-to-vector technique that leverages both global and local statistics of a corpus in order to come up with a principled loss function which uses both these. 
GLoVe\cite{pennington2014glove} can better represent the semantic meaning of a word and produce a vector space with meaningful substructure.
\subsection{BiLSTM\cite{650093}}
The idea of BiLSTM first came up in Schuster et al.(1997)\cite{650093}. The basic concept of this model is that, from human perspective, understanding a time series data should not only be forward but also backward. Thus, while building a model, we should also consider backward. BiLSTM\cite{650093} model consist of 2 LSTM \cite{6795963} : one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs\cite{650093} effectively increase the amount of information available to the network, improving the context available to the algorithm.

\subsection{Bert\cite{devlin2019bert}}
Bert, Bidirectional Encoder Representations from Transformers (7), has been widely used in NLP and showed a great performance in different task. The idea behind Bert\cite{devlin2019bert} is based on a pre-trained transformer-based encoder to generate contextualized word embeddings that can be fine-tuned on specific downstream natural language processing tasks. Unlike BiLSTM\cite{650093} which simply concatenate the forward and backward output, Transformer can learn from both directions simultaneously. By considering the entire context of a word in a sentence, BERT\cite{devlin2019bert} can capture complex linguistic phenomena and improve the accuracy of NLP applications.

\section{\bf Corpus Analysis \& Method}
\section{\bf Experiments}
\section{\bf Discussion}
\section{\bf Conclusion}
\section{\bf Reference}

\bibliographystyle{acl_natbib} 
\bibliography{custom}



\begin{enumerate}
    \item {\bf Introduction} (1/2--1 page): Motivate your work, state the problem statement clearly (inputs and outputs), and summarise your key contributions.  Optional to include are a concluding textual navigation paragraph, and/or running (microanalysis) example. 
    \item {\bf Related Work / Background} (1/2--1 1/2 pages): Implementation and experimentation that your group did in the project should be based on others' experiences.  Relate these relevant works to show that you are aware of best practices, and how your work builds upon them.  Your report can call attention to gaps in the related work to motivate your work to as innovative and filling in knowledge that is lacking.  This section typically features many citations to prior work and footnote references\footnote{Such as this one: \url{http://nlpprogress.com/}.} to online datasets or software.
    \item {\bf Corpus Analysis \& Method} (1--2 pages): Describe the different key useful approaches that yielded interesting findings here.  You need not include all of your group's work if certain branches did not prove useful; those you can mention in an appendix.  Describe the preprocessing, data collection, and the main methods used. You may also refer to your group's software repository in a footnote, if you host it online\footnote{An example would be a GitHub repository such as \url{https://github.com/knmnyn/cs4248-2120}. If you provide one, please ensure that you have at least a minimally-documented {\tt README.md} file and organize your repository accordingly}. 
    \item {\bf Experiments} (1--2 pages): This section gives the main experimental settings, such as the corpus used, (macroscopic) evaluations metrics and baselines first; then proceeds to show the key performance evaluation experimental results, usually through figures, tables or charts.  Interpret the data in these artefacts as prose explanations in the body text.
    \item {\bf Discussion} (1--3 pages): Enumerate 2--3 specific research questions and your group's answer that give more depth and analysis to the main results.  These can describe performance aspects to sub-populations of input or intended users; time, memory and compute costs and scaling; micro-analysis of specific input instances.  At least one question should be related to the natural language aspect (in contrast to general machine learning) of your project and corpora.
    \item {\bf Conclusion}: (1/4--1/2 page): This section is often abused as another chance to repeat the abstract or the introduction.  Use this section instead to help summarize and lend insight to the reader, in light that they now have read the contents of the other sections.  Limitations of your project, future directions also feature in this final section.
\end{enumerate}

\noindent Aside from these sections, there will be a short 100--200 word abstract at the beginning of the containing a summary of the work accomplished.  The abstract usually contains a clear task statement, highlights of experiments and key findings of the work.  

Following the eight-page maximum length report body, there is unlimited space for you to include materials --- an ethical statement, references and appendices (in that order). 

It is recommended that you start with this format and permute it to your liking.

%%%%% 

\section{Marking Rubric}

The marking of the project report follows a similar format to the {\it Intermediate Update}: Presentation, Content, and Miscellaneous.  We will mark out of a total maximum mark of 100.  Your grade and comments on the marking will be made available by Canvas Gradebook.

\paragraph{Report Presentation (25\%):}

\begin{itemize}
    \item Motivation: 
    \begin{itemize}
        \item Does the report clearly outline of goals and questions addressed?
        \item Is the motivation for your task clear, plausible and rational?  
        \item Is the problem statement well-defined using appropriate NLP terminology? 
        \item Does the report state the importance, usefulness, benefits of the work and the results?
    \end{itemize}
    \item Structure
    \begin{itemize}
        \item Does the report content flow logically? 
        \item Is it sufficiently well-organized to omit information that should be common knowledge to your peers?  
        \item Do you relegate less important information to an appropriate location (backmatter, software repository, footnote)? 
    \end{itemize}
    \item Visualization
    \begin{itemize}
        \item Does the report use appropriate figures, plots, and tables to justify preprocessing steps, design decisions, motivating discussions and explanations?
    \end{itemize}
    \item Presentation (more important)
    \begin{itemize}
        \item Does the prose, references, sections and visuals all complement each other in describing the logical flow?
        \item Is any corpus analysis (exploratory data analysis) done purposefully, to motivate model or experimental design?
        \item Are any visuals appropriately-sized, captioned and legible? Do they serve to better explain the material than an equivalently-sized block of prose text?
        \item Do you correctly follow the formatting instructions, length limitations and submission rules?
    \end{itemize}
    Do not just report numbers, but illustrate (with figures, tables), and explain them. Do not assume that your audience knows what your numbers mean.
    
\end{itemize}

\paragraph{Report Content (60\%):}

\begin{itemize}
    \item Originality 
    \begin{itemize}
        \item What are the original elements done in the project? (It’s not necessary that no group has done your task before, but your report needs to reflect your ability to think analytically and contribute novel analysis.)
        \item Do you articulate how your work is novel in light of the prior work?
    \end{itemize}
    \item Relevance 
    \begin{itemize}
        \item How strongly connected is the project to this course? 
        \item Do you use core concepts of NLP taught from class?
    \end{itemize}
    \item Related Work
    \begin{itemize}
        \item How strongly connected is the project to this course? 
        \item Do you use core concepts of NLP taught from class?
        \item Do you present a study of related work to the task? (Formal academic references, useful web articles and posts material, and other related work should be considered in this aspect. Remember to cite explicitly.)
    \end{itemize}
    \item Technical Justification (more important): 
    \begin{itemize}
        \item Is your technical approach suitable to try to solve your proposed problem? 
        \item Is your technical approach valid for your task and dataset? 
        \item Are there technical flaws in the execution of the approach? 
        \item Do you describe the data / corpora that you collected in an appropriate manner? (Self-annotated data may need evidence that the annotations are replicable; i.e., interannotator agreement)  
        
        \item Are evaluations performed with the appropriate metrics and correctly interpreted?
    \end{itemize}
    \item Implementation (more important): 
    \begin{itemize}
        \item Did you implement multiple models (baseline, and best)?  
        \item Do you cleanly delineate what your group members coded as original work from public library or code repositories you used from others?
        \item Did you implement or use the models correctly? 
        \item Did you tune them appropriately, where resources allowed?
    \end{itemize}
    \item Model Evaluation (more important): 
    \begin{itemize}
        \item Do you address both macroscopic, dataset-wide level performance (e.g., F1 measures) as well as microscopic, individual instance level performance (careful error analysis with diagnosis)?
        \item Do you demonstrate improvement in performance from your model to another, such as a baseline model? (A baseline model may be an implementation of a simpler model or version of your model, or referenced from other literature --- make sure to give appropriate citations). 
    \end{itemize}
    Note that your performance need not be very high (e.g., 90\%) if your data problem is hard. But you should show improvement over some baseline approach. This includes conscientious efforts to improve performance.
    \item Results Interpretation (10\%): How well are the evaluation results described and interpreted. 

    \begin{itemize}
        \item Error analysis: Explain, with evidence, why the model may be performing poorly (or not as good as you wish).  
        \item Do you justify technically why your model is good or has improved?  I.e., rationalize your approach’s performance effectiveness. 
        \item Future improvements: Discuss how you may further improve your model. 
    \end{itemize}
    You do not have to implement or test all your ideas, if too infeasible. Though discussing them helps to show your grading staff that you have good and valid ideas.
\end{itemize}

\paragraph{Miscellaneous (15\%):}

\begin{itemize}
    \item Reproducibility
    \begin{itemize}
        \item Is the technical approach described clearly and sufficiently detailed for a peer to replicate?  Is the evaluation method described clearly and detailed enough for a peer to replicate? 
        \item Is your source code well-organized and any ancillary materials well documented?
        \item Are your results easy to replicate by running documented commands or executing a notebook?
    \end{itemize}
    \item Limitations
    \begin{itemize}
        \item Do you state the principal limitations of your work, such as the important aspects of the problem domain, and how these factors might be mitigated?
    \end{itemize}
    \item Backmatter
    \begin{itemize}
        \item Do you use the backmatter and supplemental materials (website, source code repository) effectively to complement the formal report body?
        \item Are your references bibliographically complete?     
        \item Did your group appropriately fill out the {\it Statement of Independent Work}? 
        \item Did you properly acknowledge and document how AI tools played an appropriate role in your experimentation, coding and report?
    \end{itemize}
    
\end{itemize}

\paragraph{Late policy.} Please refer to the CS4248 website for late policy, in Canvas >> Pages >> Grading.  In general, our course's late policy is harsh to help our instructing staff mark in an efficient manner.  To ensure your group does well, please turn in your report on time (.PDF version to the Canvas Assignment) by the deadline.  If you envision that your group cannot meet the deadline and you wish to seek an extension, please do so well in advance of the deadline, and not after the deadline.

%%%%% 

\section{Report Formatting}
\label{s:format}

This document is a \LaTeX{} sourced document, a common typesetting system used in research communities, and commonly used for many conferences for natural language processing research.  As such we are using this typesetting system and the formatting (style) files common to best practices for communicating NLP research and outcomes.  This template is hosted on the third-party cloud-based \LaTeX{} typesetting site, Overleaf, which you may decide to use if you'd like.  Note that NUS has a sitewide license to this product so you can get professional features when using an NUS email address (e.g., ones ending in {\tt u.nus.edu} for free.

To be clear, it is {\bf not necessary} for your group to use \LaTeX{} to typeset your final report.  You may use other software and reproduce (most of)\footnote{In other software, it may be difficult to reproduce the margin line numbering, this is ok to omit.} the template following the styles noted below.  

\begin{enumerate}
    \item If you are using a system other than \LaTeX{} (e.g., MS Word, Open Office, LibreOffice, Apple Pages), you will want to follow the guidelines in the rest of this section for the report format.  
    \item If you are using Overleaf/\LaTeX{}, you should simply be able to use the logical formatting tags directly --- which should have been configured properly by the inclusion of the {\tt acl.sty} and {\tt acl\_natbib.bst} style files --- and ignore the rest of this section. Please see the \LaTeX{} source of this document for comments on other packages that may be useful. 
\end{enumerate}

Format your report to two columns to a page, A4 sized page only, in the manner these instructions are formatted. The exact dimensions for a page on A4 paper are:

\begin{itemize}
\item All (Left, right, top and bottom) margins: 2.5 cm
\item Column width: 7.7 cm
\item Column height: 24.7 cm
\item Gap between columns: 0.6 cm
\end{itemize}

For reasons of uniformity, Adobe's {\bf Times Roman} font should be
used. 

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
document text & 11 pt  &\\
captions & 11 pt & \\
abstract text & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Final Report Font guide. Captions generally should be self-sufficient to read the table or figure independently of the prose text of the report.  Use bottom and top rulelines for proper formatting.}
\end{table}

\subsection{The First Page}
\label{ssec:first}

Center the title, author's name(s) and affiliation(s) across both
columns. Do not use footnotes for affiliations. Use the
two-column format only when you begin the abstract.

{\bf Title}: Place the title centered at the top of the first page, in
a 15-point bold font. (For a complete guide to font sizes and styles,
see Table~\ref{font-table}) Long titles should be typed on two lines
without a blank line intervening. Approximately, put the title at 2.5
cm from the top of the page, followed by a blank line, then each of your groupmates' NUS student IDs,  Replace the {\tt XX} and {\tt YY} placeholders with your two-digit Group ID (e.g., ``01'') and project mentor's name.  Do not format title and section headings in all
capitals as well except for proper names (such as ``BLEU'') that are
conventionally in all capitals.  The affiliation should an electronic mail address for at least one contact student.  

Start the body of the first page 7.5 cm from the top of the page.

{\bf Abstract}: Type the abstract at the beginning of the first
column. The width of the abstract text should be smaller than the
width of the columns for the text in the body of the paper by about
0.6 cm on each side. Center the word {\bf Abstract} in a 12 point bold font above the body of the abstract. The abstract should be a concise summary of the general thesis and conclusions of the paper. It should be no longer than 200 words. The abstract text should be in 10 point font.

{\bf Text}: Begin typing the main body of the text immediately after
the abstract, observing the two-column format as shown in 
the present document. Do not include page numbers.

{\bf Indent} when starting a new paragraph. Use 11 points for text and 
subsection headings, 12 points for section headings and 15 points for
the title. 

\subsection{Sections}

{\bf Headings}: Type and label section and subsection headings in the
style shown on the present document.  Use numbered sections (Arabic
numerals) in order to facilitate cross references. Number subsections
with the section number and the subsection number separated by a dot,
in Arabic numerals. Do not number subsubsections.

\subsection{Citations}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Output} & \textbf{natbib command} \\
\hline
\citep{Gusfield:97} & \verb|\citep|\\
\citealp{Gusfield:97} & \verb|\citealp| \\
\citet{Gusfield:97} & \verb|\citet| \\
\citeyearpar{Gusfield:97} & \verb|\citeyearpar| \\
\hline
\end{tabular}
\caption{\label{citation-guide}
Citation commands supported by the {\tt acl.sty} style file.
The style is based on the natbib package and supports all natbib citation commands.}
\end{table}

Citations are bibliographic references to scholarly works.  You may include references to documentary web pages, blog posts, reports, pre-prints as well.  Note that references to general webpages or software (packages) as URLs are more appropriately given as footnotes.

Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\subsection{Backmatter}

Backmatter are other materials that follow the main report body.  They include the following:

\paragraph{References.} Your group should cite all appropriate references that you need in your report.  You may place an {\it unlimited} number of references to work that is relevant, beyond the page limit for the main report.
\nocite{Ando2005,borschinger-johnson-2011-particle,andrew2007scalable,rasooli-tetrault-2015,goodman-etal-2016-noise,harper-2014-learning}

The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{custom}
\end{verbatim}
\end{quote}

Many papers in natural language processing come from the ACL Anthology, the digital library for NLP, which Min ran for many years.  You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}

\paragraph{Statement of Independent Work.} Your group must include this section, to declare whether your group followed class policy.  Refer to the example in this document's backmatter.

\paragraph{Ethical Statement.} An optional, unnumbered section. 
Refer to the example in this document's backmatter.

\paragraph{Acknowledgements.} An optional, unnumbered section. 
Refer to the example in this document's backmatter.

\paragraph{Appendices.} You are also allowed {\it unlimited pages} for appendices, but be aware that your teaching staff is not obligated to read or acknowledge these sources.

Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

%%%%% 

\clearpage

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\section*{Acknowledgements}

Place any acknowledgements here.  You can thank any people you contacted or sources that you used that are not bibliographic in nature.

This document has been adapted from the ACL Rolling Review Template (ACL ARR) by Min-Yen Kan.  You may find the original template, which NUS has also contributed to in the past, here: \url{https://www.overleaf.com/latex/templates/acl-rolling-review-template/jxbhdzhmcpdm}.  We have omitted much of the original document to cut down on verbiage.

Our final report grading rubric is based on a merger of guidelines from component courses CS5228 Knowledge Discovery and Data Mining and CS3244 Machine Learning.

\section*{Statement of Independent Work}

{\it You {\bf must} include the text of the two statements below in your group's submitted work.  Digitally sign your submission using your Student Numbers (starting with A...; N.B., not your NUSNET email identifier). This is a required section and is not part of the main body (doesn't count towards your page limit).} \\

\noindent 1A. Declaration of Original Work. By entering our Student IDs below, we certify that we completed our assignment independently of all others (except where sanctioned during in-class sessions), obeying the class policy outlined in the introductory lecture. In particular, we are allowed to discuss the problems and solutions in this assignment, but have waited at least 30 minutes by doing other activities unrelated to class before attempting to complete or modify our answers as per the class policy. \\

We have documented our use of AI tools (if applicable) in a following table, as suggested in the NUS AI Tools policy\footnote{\url{https://libguides.nus.edu.sg/new2nus/acadintegrity}, tab ``AI Tools: Guidelines on Use in Academic Work''}.  This particular document did not use any AI Tools to proofcheck and was constructed and edited purely by manual work.\\

\noindent 1B. Exception to the Class Policy. We did not follow the CS4248 Class Policy in doing this assignment. This text explains why and how we believe we should be assessed for this assignment given the circumstances explained. \\

\noindent Signed, [Enter your Axxx Student IDs here]

\section*{Ethical Statement}

The optional ethical statement is an unnumbered section that comes after the references.  Most projects may not need to include such a statement, but we include it here, as it is important to be aware that NLP research and experimentation needs to be conducted in an ethically acceptable manner.  

It describes any pertinent issues with respect to the NL technology being described in the project work.  These could include dual-use, data quality discussions, compute requirements, fair pay for annotators and evaluators, among other factors.  

You may read more about these issues by reading the {\it Guidelines for Responsible NLP Research}\footnote{\url{https://aclrollingreview.org/responsibleNLPresearch/}} 
and consulting works on the ACL Ethics Reading List\footnote{\url{https://github.com/acl-org/ethics-reading-list}}.

\appendix

\section{Example Appendix}
\label{sec:appendix}

Optional appendices are the last item in the report.

If your group's report is too long, working to best structure the core of the report (instead of technical details) in the main body and relegating details for replication in appropriate appendices is key.  

Since you have unlimited pages for appendices, you can afford to make any plots or result tables larger in the appendices, but do ensure that key results are in the report's main page limit rather than relegated here.

\section{Project Frequently Asked Questions (FAQ)}

{\it This section is sourced from the CS3244 Machine Learning module's Project FAQ.}

\small

\begin{enumerate}
    \item {\it Q: Just to be sure, for our project work, can we code in any language (i.e. R) other than python? } \\
    A: Yes. 
    
    \item {\it Q: Does the project difficulty matters for the grading, e.g. taking the {\tt easy} dataset versus doing something tagged as {\tt hard}?  Would we be graded based on the novelty or ``importance'' of the use case/problem our group comes up with?} \\ 
    A: We aren't looking at technical complexity when grading the project. We state the notional technical difficulty of project dataset to help your team decide which type of project to take on.  More difficult datasets usually involve more specific  preprocessing, data normalization and usually (much) larger compute costs in manipulating large-scale data.
    \begin{itemize}
        \item What we are looking at is the learning that comes out of engaging in the project. We want to see you twist your mind and come up with interesting approaches to the problem of choice.
        \item We also do not place heavy emphasis on metrics like accuracy, log loss, precision, recall, etc.. We'd care more about questions like ``why did you use XYZ Metric over ABC Metric for this problem?''. Getting a +0.5 accuracy boost doesn't matter as much as why you chose to do ABC Technique that brought about that accuracy boost in the first place.  \item We care more about how you communicate your findings to us in an interesting way like your analysis, your wins, your losses (pun not intended), etc.. That way, it shows us that you gained valuable experience from this project that you can apply to future projects.
        \item You are allowed to explore models not covered in class at your own discretion. We only teach you the fundamentals in hopes of making you comfortable with the math/concepts involved. Beyond that, you can look at more complex models and techniques not taught in CS4248 for your projects. But again, complexity is not the focus, communication and understanding the 2W1H (why, what, how) are.
        \item There isn't any true ``novelty'' in these projects {\it per se}. They are popular benchmarks found in the real world with increasing difficulty of use. We want you to have your own unique spin to these solutions (please do not copy-paste/plagiarise someone else's code from online) and present them in a way you and we (i.e. the teaching staff) understand.  
    \end{itemize}
    These projects are for you in the long run, not us. Hope this helps.

    \item {\it Q: What local compute do we have access to for our projects?}\\
    A: Please take note that our class' reservation for compute nodes in SoC has now taken effect (from 26 Jan until 15 Apr).  If your groups find it useful, you may start using it if you have previously registered an SoC UNIX ID. The nodes you may work with are {\tt xgpf0--6} (i.e., use the command {\tt ssh xgpf0.comp.nus.edu.sg} within SoC's network to reach the first of seven available servers).  You may use other nodes but these compute resources are exclusively for our class' use.  If you use these resources, please self-regulate and use a maximum of one (1) node per group.  It is unfair if one group hogs all of the resources and makes the resources unavailable to others.  Please be respectful and mindful that your group is one of many in our cohort and all groups should be able to utilize some of these resources.

    % \item {\it Q: If there are more than one idle node remaining, is it fair for a group to use more than one node to speed up their training and free up the resources sooner for the others?} \\
    % A: In general you might try to use {\tt nice} or other commands to change the priority of your jobs.  But in the case of GPUs they are either utilized or not and such job priority scheduling doesn't work.   If you can carefully monitor the GPUs utilization and ensure there are at least 1--2 other machines in the 6 machines in the {\tt xgfp0--6} cluster that are free, we are fine with a group using more than one.  But you should not schedule very long jobs that hog group resources.

    \item {\it Q. Have any advice for experimentation?}\\
    A: Sure. We recommend staging your experiments' time to execute to fit your working style.  For example, we recommend having 3 granularities of time for your experiment execution: immediate (finishes within 1--3 minutes), coffee/tea break (finishes within 30-60 mins), overnight (as the name implies).  Based on some initial runs, you should develop a good estimation for how long your pipeline takes for a certain data scale, and retrofit/sample data from your dataset to fit accordingly.

    {\bf Immediate} experiments should just to check that your code works with a tiny toy dataset without faults and to assess whether an experimental setting can be escalated to the next granularity.  

    {\bf Coffee / Tea Break} experiments test those runs from the Immediate scale that reach your standard for trying on a larger dataset.  These experiments can be set to run on a server with a medium-sized dataset that can complete independently while you are eating a meal, or taking a break to do other work or play.  These validate your ideas on larger scale datasets without committing to training an entire dataset without knowing whether the results point appropriately in the proper direction – Min has seen many times that the "math works out  (e.g., shape of matrices are fine) but which the computation is garbage (e.g., one off indexing errors) – so this scale mitigates this.  These scale experiments need to be followed up with analysis to ensure that the results are as expected and appropriate.

    {\bf Overnight} (or longer) experimentation runs your training or testing at scale, for production or for final presentations or reports.  Try not to do this scale of experimentation without having a good reason to believe it will succeed (i.e., don't run a large-scale experiment to try something out; you should have done that at the Coffee / Tea Break scale instead).

    \item {\it Q: Is model performance and using and getting state-of-the-art performance an important output of our project?}\\
    A: Generally, no.  Good projects explain and teach, rather than just show good results.  It is better to use simpler models where you can show that your understanding of the model, features, corpus and evaluation metrics interact to lead to the performance levels you observe.  Merely swapping in a newer more advanced model and getting better results doesn't merit this understanding.  Good and replicable results are nice-to-have in terms of grading criteria, but not must-haves.

    \item {\it Q: Just want to check if our group’s understanding of an ablation study is correct.  If we have around 20 features that we engineered and a baseline model of previously proposed features, how do we efficiently conduct an ablation study?  We are thinking of grouping the 20 features we have into a few groups to turn them on/off for the study. Is this the correct way? Should our angle start out from all features included to removing parts of it, or start from the baseline model and add features, since it's an ``ablation'' study.} \\
    A: Yes, that would be appropriate. You can turn off a feature group, or some pre-/post- processing and see what the effects are. Before you do that, your team should hypothesize what you think would happen. This can help you hone your sense of understanding. A scientist does experimentation with a hypothesis in mind.  Generally you have a final model and you turn off certain features to study their (negative) impact on your final model. This allows you to argue for the necessity of all feature groups in your model.

    \item {\it Q: I have some questions regarding the ablation studies we need to do for the project.  Based on my understanding, the aim of ablation studies is to understand how our existing system (i.e. the feature engineering techniques+the models) work.  And I remember you saying that it is about having hypothesis and finding ways to test them. 
    So my first question is, do we have to have some form of "removal" to conduct ablation analysis? Or is it ok to analyse without ``removal''?}\\
    A: Yes that's correct.  No you need not (always) do a removal (ablation) for analysis. It is just a common form. Both additive and ablative (removal) studies of feature classes are common.  Ablation studies often form the basis for arguing that the model cannot have any of its elements removed without damaging a performance metric.

    \item {\it Q: If the aim is to understand how our existing system works, how do we control the factors of the experiment? For example, if feature engineering technique A works well with a model, and we wanna know what is in A that makes this succeed, so we modify technique A into its variant C (with one feature removed), and then here's the question:  do we feed the input processed using C into the model trained using A and see the change of results, or do we train a new model of the same structure on this input processed using C and then compare the results with inputs processed by A feeding into model trained using A?}\\
    A: Yes, sometimes we refer to C as ``C: Model A–<some feature>''. The second method, train a new model.  We compare the performance macroscopically (whole dataset, e.g., accuracy, $F_1$) against the system trained with the output from A.

    \item {\it Q: It was mentioned at the start of the sem that there was no real novelty in our projects, but there was novelty of project in the project presentation rubrics. Could you please clarify what novelty in the marking rubric truly stands for?.}\\
    A: Indeed with many of our curated datasets, there has been much (informally) published past work.  You should find, read and cite any past work in your pre-recorded presentation and add these links to the supplemental materials you prepare, so that the instruction staff can check accordingly. You should validate (by replication) performance figures from other papers or posts. You do original work by going beyond what others have reported.  There are many ways you can go beyond the past work in your analyses and subsequent iterative questioning and answering of your project work.  Don't concentrate just on performance metrics but look for ways to connect what you've learned in lecture with your project.

    Examples include:
    \begin{itemize}
        \item How do changes in your model architecture affect performance?  Not just at the macro performance metrics but for individual (micro) and groups (meso) problem instances?
        \item How does changing some input instances minimally change the results for better or worse?
        \item Which features or model paradigm designs contribute the most towards performance and, more crucially, why?
        \item Why do certain models do better at certain instances and not for others?
    \end{itemize}
    
\end{enumerate}

\section{Version History}

Note: This list is in reverse chronological order.  Timestamps follow the {\it yymmdd} format.
\begin{enumerate}
    \item 230312 --- Updated for Canvas and AI Tools policy.
    \item 220324 --- Updated report length and data analysis sections.
    \item 220320 --- Initial distribution.
\end{enumerate}

\end{document}
