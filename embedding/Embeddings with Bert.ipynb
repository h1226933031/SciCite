{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ac28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 768)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, AutoModel, BertTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "class SciCiteEmbedding:\n",
    "    def __init__(self, glove=False, word2vec=False, elmo=False, bert=False):\n",
    "        self.word2vec = word2vec\n",
    "        self.glove = glove\n",
    "        self.elmo = elmo\n",
    "        self.bert = bert\n",
    "        self.ndim = None\n",
    "\n",
    "    def embed(self, X_train):  # return a 3D numpy array. shape: [samples, max_length, embed_dim]\n",
    "        embed_list = []\n",
    "        if self.word2vec:  # time-consuming and could miss some tokens that do not appear in word2vec keys.\n",
    "            # if word2vec model has not been dowloaded, plz run this:\n",
    "            # path = gensim.downloader.load(\"word2vec-google-news-300\", return_path=True)\n",
    "            word2vec_model_path = \"./word2vec/word2vec-google-news-300.gz\"\n",
    "            w2v = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)\n",
    "            max_length = 0\n",
    "            for token_list in X_train:\n",
    "                np_list = []\n",
    "                for token in token_list:\n",
    "                    try:\n",
    "                        np_list.append(w2v[token])\n",
    "                    except:\n",
    "                        continue\n",
    "                max_length = max(max_length, len(np_list))\n",
    "                embed_list.append(np.array(np_list))\n",
    "            embed_list = [np.concatenate([array, np.zeros((max_length-array.shape[0], 300))], axis=0) for array in embed_list]\n",
    "            return np.array(embed_list)  # [samples, max_length, 300]\n",
    "\n",
    "        elif self.glove:\n",
    "            max_length = max([len(token_list) for token_list in X_train])\n",
    "            glove_model_path = './GloVe/glove2word2vec_model.sav'  # glove embed dimension=100\n",
    "            glove = pickle.load(open(glove_model_path, 'rb'))\n",
    "            for token_list in X_train:\n",
    "                np_list = []\n",
    "                for token in token_list:\n",
    "                    try:\n",
    "                        v = glove[token]\n",
    "                    except:\n",
    "                        continue\n",
    "                    np_list.append(v)\n",
    "                embed_list.append(\n",
    "                    np.concatenate([np.array(np_list), np.zeros((max_length - len(token_list), 100))], axis=0))\n",
    "            embed_array = np.array(embed_list)  # [n_sample, max_length, ndim=512]\n",
    "            return embed_array\n",
    "\n",
    "        elif self.elmo: # elmo_dim = 512\n",
    "            elmo_options_file = \"./elmo/elmo_2x2048_256_2048cnn_1xhighway_options.json\"\n",
    "            elmo_weight_file = \"./elmo/elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5\"\n",
    "            elmo_model = Elmo(elmo_options_file, elmo_weight_file, 1, dropout=0)\n",
    "            character_ids = batch_to_ids(X_train)\n",
    "            embeddings = elmo_model(character_ids)['elmo_representations'][0]\n",
    "            return embeddings.detach().numpy()  # [n_sample, max_length, ndim=512]\n",
    "\n",
    "        elif self.bert:#dim = 768\n",
    "            model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "            # kiv: big file to download\n",
    "            tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "            character_ids = tokenizer.batch_encode_plus(X_train, return_token_type_ids= False,is_split_into_words=True,padding=True)\n",
    "            tokens_tensor = torch.tensor(character_ids['input_ids'])\n",
    "            segments_tensor = torch.tensor(character_ids['attention_mask'])\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens_tensor, segments_tensor)\n",
    "            hidden_states = outputs[2][0].tolist()\n",
    "            embeddings = []\n",
    "            for element in hidden_states:\n",
    "                for array in element:\n",
    "                    embeddings.append(np.array(array))\n",
    "            return np.array(embeddings)\n",
    "\n",
    "\n",
    "test = [['ok', ',', 'fine', 'i', 'will', 'check', 'it', 'later', '.'],\n",
    "        ['love', 'you', '!'],\n",
    "        ['i', 'think', 'data', 'preprocessing', 'is', 'so', 'complicated', '.']]\n",
    "\n",
    "embed_model = SciCiteEmbedding(bert=True)  # pick an embedding method\n",
    "embed_array = embed_model.embed(test)\n",
    "np.save('../data/a_simple_test.npy', embed_array)\n",
    "print(embed_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db00d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
